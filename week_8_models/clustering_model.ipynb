{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the baseline model, located in `../baseline_models/clustering_model.ipynb`, \n",
    "- we extracted `sentences_df.csv` dataset from the original `ner_dataset.csv',  \n",
    "- applied two vectorization algorithms, `CountVectorizer` and `TfidfVectorizer`, to convert _shortened sentences_ (original sentences with stopwords removed) to vectors, \n",
    "- and finally, used the KMeans clustering algorithm to cluster sentences. \n",
    "\n",
    "The Silhouette scores of the resulting clusterings were extremely low (less than $0.1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, \n",
    "- we improve the model by adding **lemmatization** and **dimension reduction** layers.  \n",
    "- In addition to KMeans, we also explored other clustering models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset `sentences_v1.csv`, created in the baseline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "sentences_df = pd.read_csv('../datasets/sentences_v1.csv')\n",
    "extended_df = pd.read_csv('../datasets/extended_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'Sentence Length', 'Sentence#', 'Content',\n",
       "       'Tagged Words', 'Shortened Sentences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns `'Unnamed: 0.1', 'Unnamed: 0'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence Length</th>\n",
       "      <th>Sentence#</th>\n",
       "      <th>Content</th>\n",
       "      <th>Tagged Words</th>\n",
       "      <th>Shortened Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>['Thousands', 'of', 'demonstrators', 'have', '...</td>\n",
       "      <td>['London', 'Iraq', 'British']</td>\n",
       "      <td>Thousands  demonstrators  marched  London  pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>['Families', 'of', 'soldiers', 'killed', 'in',...</td>\n",
       "      <td>['Bush']</td>\n",
       "      <td>Families  soldiers killed   conflict joined  p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>['They', 'marched', 'from', 'the', 'Houses', '...</td>\n",
       "      <td>['Hyde', 'Park']</td>\n",
       "      <td>marched   Houses  Parliament   rally  Hyde Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>['Police', 'put', 'the', 'number', 'of', 'marc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police   number  marchers  10,000  organizers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>['The', 'protest', 'comes', 'on', 'the', 'eve'...</td>\n",
       "      <td>['Britain', 'Labor', 'Party', 'English', 'Brig...</td>\n",
       "      <td>protest comes   eve   annual conference  Brit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence Length  Sentence#  \\\n",
       "0               24          1   \n",
       "1               30          2   \n",
       "2               14          3   \n",
       "3               15          4   \n",
       "4               25          5   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ['Thousands', 'of', 'demonstrators', 'have', '...   \n",
       "1  ['Families', 'of', 'soldiers', 'killed', 'in',...   \n",
       "2  ['They', 'marched', 'from', 'the', 'Houses', '...   \n",
       "3  ['Police', 'put', 'the', 'number', 'of', 'marc...   \n",
       "4  ['The', 'protest', 'comes', 'on', 'the', 'eve'...   \n",
       "\n",
       "                                        Tagged Words  \\\n",
       "0                      ['London', 'Iraq', 'British']   \n",
       "1                                           ['Bush']   \n",
       "2                                   ['Hyde', 'Park']   \n",
       "3                                                NaN   \n",
       "4  ['Britain', 'Labor', 'Party', 'English', 'Brig...   \n",
       "\n",
       "                                 Shortened Sentences  \n",
       "0  Thousands  demonstrators  marched  London  pro...  \n",
       "1  Families  soldiers killed   conflict joined  p...  \n",
       "2   marched   Houses  Parliament   rally  Hyde Pa...  \n",
       "3  Police   number  marchers  10,000  organizers ...  \n",
       "4   protest comes   eve   annual conference  Brit...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sent = sentences_df['Shortened Sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop an **invaid** sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8411    NaN\n",
       "Name: Shortened Sentences, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_sent[shortened_sent.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence Length</th>\n",
       "      <th>Sentence#</th>\n",
       "      <th>Content</th>\n",
       "      <th>Tagged Words</th>\n",
       "      <th>Shortened Sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>8409</td>\n",
       "      <td>8409</td>\n",
       "      <td>33</td>\n",
       "      <td>8410</td>\n",
       "      <td>['A', 'STATE', 'Official', 'carrying', 'off', ...</td>\n",
       "      <td>['Dome', 'of', 'the', 'Capitol']</td>\n",
       "      <td>STATE Official carrying   Dome   Capitol met ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8410</th>\n",
       "      <td>8410</td>\n",
       "      <td>8410</td>\n",
       "      <td>34</td>\n",
       "      <td>8411</td>\n",
       "      <td>['As', 'the', 'place', 'of', 'meeting', 'was',...</td>\n",
       "      <td>['midnight', 'State', 'Official', 'Dome', 'of'...</td>\n",
       "      <td>place  meeting  lonely   time midnight ,  St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8411</th>\n",
       "      <td>8411</td>\n",
       "      <td>8411</td>\n",
       "      <td>1</td>\n",
       "      <td>8412</td>\n",
       "      <td>['The']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8412</th>\n",
       "      <td>8412</td>\n",
       "      <td>8412</td>\n",
       "      <td>28</td>\n",
       "      <td>8413</td>\n",
       "      <td>['Ghost', 'replied', 'that', 'he', 'had', 'not...</td>\n",
       "      <td>['State', 'Official']</td>\n",
       "      <td>Ghost replied     eaten  ,     explaining  sit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  Unnamed: 0  Sentence Length  Sentence#  \\\n",
       "8409          8409        8409               33       8410   \n",
       "8410          8410        8410               34       8411   \n",
       "8411          8411        8411                1       8412   \n",
       "8412          8412        8412               28       8413   \n",
       "\n",
       "                                                Content  \\\n",
       "8409  ['A', 'STATE', 'Official', 'carrying', 'off', ...   \n",
       "8410  ['As', 'the', 'place', 'of', 'meeting', 'was',...   \n",
       "8411                                            ['The']   \n",
       "8412  ['Ghost', 'replied', 'that', 'he', 'had', 'not...   \n",
       "\n",
       "                                           Tagged Words  \\\n",
       "8409                   ['Dome', 'of', 'the', 'Capitol']   \n",
       "8410  ['midnight', 'State', 'Official', 'Dome', 'of'...   \n",
       "8411                                                NaN   \n",
       "8412                              ['State', 'Official']   \n",
       "\n",
       "                                    Shortened Sentences  \n",
       "8409   STATE Official carrying   Dome   Capitol met ...  \n",
       "8410    place  meeting  lonely   time midnight ,  St...  \n",
       "8411                                                NaN  \n",
       "8412  Ghost replied     eaten  ,     explaining  sit...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.iloc[8409: 8413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sent.drop(8411, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_sent.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorization and Lemmatization classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/yinghu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `LemmaTokenizer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code is adapted from sklearn: https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in doc.lower().split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is to test the `LemmaTokenizer` class. There is no need to initialize tokenizer here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47958, 28760)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer_bow = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "#tokenized_sent = vectorizer_bow.fit_transform(shortened_sent)\n",
    "#tokenized_sent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence is represented by a $28760$ dim vector. Oddly, the dimension is increased after using lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47958, 27706)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized_sent_wolema = CountVectorizer().fit_transform(shortened_sent)\n",
    "#tokenized_sent_wolema.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word2vec for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download the full model \n",
    "# import gensim.downloader as api \n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimension reduction classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: `CountVectorizer` with `tokenizer=LemmaTokenizer()` + `KMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When `n_cluster=2`, Silhouette Coefficient is 0.28244613563900284 for `random_states=1, 5, 10, 42`\n",
    "- When `n_cluster=3`, Silhouette Coefficient is 0.17099173313074406 for `random_states=0, 1`\n",
    "\n",
    "This is a substantial improvement comparing to the baseline model, which has Silhouette Coefficient less than 0.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sent_bow = CountVectorizer(tokenizer=LemmaTokenizer()).fit_transform(shortened_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47958, 28760)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_sent_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and evaluate K-Mean model\n",
    "\n",
    "**We terminated the execution early.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters = 3, random_state = 0: Silhouette Coefficient: 0.17099173313074406\n",
      "n_clusters = 4, random_state = 0: Silhouette Coefficient: 0.12557863515767284\n",
      "n_clusters = 5, random_state = 0: Silhouette Coefficient: 0.09898807800970112\n",
      "n_clusters = 6, random_state = 0: Silhouette Coefficient: 0.0744724413531645\n",
      "n_clusters = 7, random_state = 0: Silhouette Coefficient: 0.059102724262040676\n",
      "n_clusters = 8, random_state = 0: Silhouette Coefficient: 0.043977833607019796\n",
      "n_clusters = 9, random_state = 0: Silhouette Coefficient: 0.05252731485513745\n",
      "n_clusters = 10, random_state = 0: Silhouette Coefficient: 0.0317145227045245\n",
      "n_clusters = 11, random_state = 0: Silhouette Coefficient: 0.03639271433403673\n",
      "n_clusters = 12, random_state = 0: Silhouette Coefficient: 0.04897327320206242\n",
      "n_clusters = 13, random_state = 0: Silhouette Coefficient: 0.026837843138244215\n",
      "n_clusters = 14, random_state = 0: Silhouette Coefficient: 0.025795734505181533\n",
      "n_clusters = 3, random_state = 1: Silhouette Coefficient: 0.17099173313074406\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m kmeans_model\u001b[39m.\u001b[39mlabels_\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_clusters = \u001b[39m\u001b[39m{\u001b[39;00mn_cluster\u001b[39m}\u001b[39;00m\u001b[39m, random_state = \u001b[39m\u001b[39m{\u001b[39;00mrandom_state\u001b[39m}\u001b[39;00m\u001b[39m: Silhouette Coefficient: \u001b[39m\u001b[39m{\u001b[39;00msilhouette_score(shortened_sent_bow, labels)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39;49mmetric, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:237\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    233\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    234\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    236\u001b[0m )\n\u001b[0;32m--> 237\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mpairwise_distances_chunked(X, reduce_func\u001b[39m=\u001b[39;49mreduce_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    238\u001b[0m intra_clust_dists, inter_clust_dists \u001b[39m=\u001b[39m results\n\u001b[1;32m    239\u001b[0m intra_clust_dists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1850\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1849\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 1850\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1852\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1854\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:2022\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   2020\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 2022\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1563\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1560\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1562\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    370\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[1;32m    371\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/utils/extmath.py:160\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    152\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    156\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m ):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39;49mtoarray()\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/scipy/sparse/_compressed.py:1062\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     y \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mT\n\u001b[1;32m   1061\u001b[0m M, N \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39m_swap(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m-> 1062\u001b[0m csr_todense(M, N, x\u001b[39m.\u001b[39;49mindptr, x\u001b[39m.\u001b[39;49mindices, x\u001b[39m.\u001b[39;49mdata, y)\n\u001b[1;32m   1063\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for random_state in [0, 1, 10, 42]: \n",
    "    for n_cluster in range(2,7):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=random_state).fit(shortened_sent_bow)\n",
    "        labels = kmeans_model.labels_\n",
    "\n",
    "        # print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\n",
    "        # print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\n",
    "        print(f'n_clusters = {n_cluster}, random_state = {random_state}: Silhouette Coefficient: {silhouette_score(shortened_sent_bow, labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters = 2, random_state = 1: Silhouette Coefficient: 0.28244613563900284\n",
      "n_clusters = 2, random_state = 5: Silhouette Coefficient: 0.28244613563900284\n",
      "n_clusters = 2, random_state = 10: Silhouette Coefficient: 0.28244613563900284\n",
      "n_clusters = 2, random_state = 42: Silhouette Coefficient: 0.28244613563900284\n"
     ]
    }
   ],
   "source": [
    "for random_state in [1, 5, 10, 42]: \n",
    "    for n_cluster in range(2,3):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=random_state).fit(shortened_sent_bow)\n",
    "        labels = kmeans_model.labels_\n",
    "\n",
    "        # print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\n",
    "        # print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\n",
    "        print(f'n_clusters = {n_cluster}, random_state = {random_state}: Silhouette Coefficient: {silhouette_score(shortened_sent_bow, labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the core sentences when `n_cluster = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "sent_dist = kmeans.fit_transform(shortened_sent_bow)\n",
    "representative_sent_idx = np.argmin(sent_dist, axis=0)\n",
    "representative_sent = sentences_df.iloc[representative_sent_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/dqjv2g8s6mgbcb781dlckr7w0000gp/T/ipykernel_54670/2726321009.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16893    ['One', 'time', 'I', 'had', 'to', 'go', 'to', 'a', 'funeral', 'at', '6', 'AM', '.']                                                                                       \n",
       "33172    ['Authorities', 'say', 'Commander', 'Victor', 'Berrones', 'died', 'Tuesday', 'when', 'gunmen', 'attacked', 'the', 'vehicle', 'in', 'which', 'he', 'was', 'traveling', '.']\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "representative_sent['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the core sentences when `n_cluster = 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "sent_dist = kmeans.fit_transform(shortened_sent_bow)\n",
    "representative_sent_idx = np.argmin(sent_dist, axis=0)\n",
    "representative_sent = sentences_df.iloc[representative_sent_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/50/dqjv2g8s6mgbcb781dlckr7w0000gp/T/ipykernel_54670/2726321009.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44758    ['Officials', 'say', 'the', 'strike', 'hit', 'a', 'Palestinian', 'militant', 'training', 'camp', '.']                                                                                                                                  \n",
       "40246    ['One', 'of', 'them', 'got', 'terribly', 'sick', '.']                                                                                                                                                                                  \n",
       "13830    ['\"', 'The', 'next', 'time', 'you', 'touch', 'a', 'Nettle', ',', 'grasp', 'it', 'boldly', ',', 'and', 'it', 'will', 'be', 'soft', 'as', 'silk', 'to', 'your', 'hand', ',', 'and', 'not', 'in', 'the', 'least', 'hurt', 'you', '.', '\"']\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representative_sent['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.14102267137398"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_dist.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (Not good): `TfidfVectorizer` with `tokenizer=LemmaTokenizer()` + `KMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using `TfidfVectorizer` results in much worse performance than `CountVectorizer` \n",
    "- **We terminated the search early.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sent_tfidf = TfidfVectorizer(tokenizer=LemmaTokenizer()).fit_transform(shortened_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47958, 28760)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortened_sent_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters = 2: Silhouette Coefficient: 0.008146341934268077\n",
      "n_clusters = 2: Silhouette Coefficient: 0.008146341934268077\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb Cell 43\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m kmeans_model\u001b[39m.\u001b[39mlabels_\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_clusters = \u001b[39m\u001b[39m{\u001b[39;00mn_cluster\u001b[39m}\u001b[39;00m\u001b[39m: Silhouette Coefficient: \u001b[39m\u001b[39m{\u001b[39;00msilhouette_score(shortened_sent_tfidf, labels)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39;49mmetric, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:237\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    233\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    234\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    236\u001b[0m )\n\u001b[0;32m--> 237\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mpairwise_distances_chunked(X, reduce_func\u001b[39m=\u001b[39;49mreduce_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    238\u001b[0m intra_clust_dists, inter_clust_dists \u001b[39m=\u001b[39m results\n\u001b[1;32m    239\u001b[0m intra_clust_dists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1850\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1849\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 1850\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1852\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1854\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:2022\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   2020\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 2022\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1563\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1560\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1562\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    370\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[1;32m    371\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/utils/extmath.py:152\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    156\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m ):\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/scipy/sparse/_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/scipy/sparse/_base.py:541\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 541\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_sparse_matrix(other)\n\u001b[1;32m    543\u001b[0m \u001b[39m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n\u001b[1;32m    544\u001b[0m other_a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(other)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/scipy/sparse/_compressed.py:533\u001b[0m, in \u001b[0;36m_cs_matrix._mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    530\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(nnz, dtype\u001b[39m=\u001b[39mupcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, other\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m    532\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matmat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 533\u001b[0m fn(M, N, np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, dtype\u001b[39m=\u001b[39;49midx_dtype),\n\u001b[1;32m    534\u001b[0m    np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, dtype\u001b[39m=\u001b[39;49midx_dtype),\n\u001b[1;32m    535\u001b[0m    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    536\u001b[0m    np\u001b[39m.\u001b[39;49masarray(other\u001b[39m.\u001b[39;49mindptr, dtype\u001b[39m=\u001b[39;49midx_dtype),\n\u001b[1;32m    537\u001b[0m    np\u001b[39m.\u001b[39;49masarray(other\u001b[39m.\u001b[39;49mindices, dtype\u001b[39m=\u001b[39;49midx_dtype),\n\u001b[1;32m    538\u001b[0m    other\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    539\u001b[0m    indptr, indices, data)\n\u001b[1;32m    541\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m((data, indices, indptr), shape\u001b[39m=\u001b[39m(M, N))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for n_cluster in range(2, 6):\n",
    "    for random_state in range(4):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=random_state).fit(shortened_sent_tfidf)\n",
    "        labels = kmeans_model.labels_\n",
    "        # print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\n",
    "        # print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\n",
    "        print(f'n_clusters = {n_cluster}: Silhouette Coefficient: {silhouette_score(shortened_sent_tfidf, labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: `CountVectorizer` with `tokenizer=LemmaTokenizer()` + `TruncatedSVD` + `KMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: `PCA` class doesn't support sparse input. Instead, we use `TruncatedSVD` (Latent semantic analysis) for dimension reduction.\n",
    "\n",
    "We drop the dimension to 100 and the performance is exactly the same as before the dimension reduction. \n",
    "\n",
    "**We terminate the searching of the best parameters early**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_sent_bow = CountVectorizer(tokenizer=LemmaTokenizer()).fit_transform(shortened_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components=100)\n",
    "shortened_sent_reducded = lsa.fit_transform(shortened_sent_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50368223, 0.03100053, 0.00577348, 0.0062948 , 0.00417825,\n",
       "       0.00403233, 0.00384653, 0.00317059, 0.00308567, 0.00299286])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.explained_variance_ratio_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666552006697911"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters = 2, random_state = 0: Silhouette Coefficient: 0.28244613563900284\n",
      "n_clusters = 3, random_state = 0: Silhouette Coefficient: 0.17099173313074406\n",
      "n_clusters = 4, random_state = 0: Silhouette Coefficient: 0.12557863515767284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#Y154sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m kmeans_model\u001b[39m.\u001b[39mlabels_\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#Y154sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#Y154sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yinghu/Documents/GitHub/glg-project/week_8_models/clustering_model.ipynb#Y154sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mn_clusters = \u001b[39m\u001b[39m{\u001b[39;00mn_cluster\u001b[39m}\u001b[39;00m\u001b[39m, random_state = \u001b[39m\u001b[39m{\u001b[39;00mrandom_state\u001b[39m}\u001b[39;00m\u001b[39m: Silhouette Coefficient: \u001b[39m\u001b[39m{\u001b[39;00msilhouette_score(shortened_sent_bow, labels)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39;49mmetric, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py:237\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    233\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    234\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[1;32m    236\u001b[0m )\n\u001b[0;32m--> 237\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49mpairwise_distances_chunked(X, reduce_func\u001b[39m=\u001b[39;49mreduce_func, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    238\u001b[0m intra_clust_dists, inter_clust_dists \u001b[39m=\u001b[39m results\n\u001b[1;32m    239\u001b[0m intra_clust_dists \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1850\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1848\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1849\u001b[0m     X_chunk \u001b[39m=\u001b[39m X[sl]\n\u001b[0;32m-> 1850\u001b[0m D_chunk \u001b[39m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[39m=\u001b[39;49mmetric, n_jobs\u001b[39m=\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1851\u001b[0m \u001b[39mif\u001b[39;00m (X \u001b[39mis\u001b[39;00m Y \u001b[39mor\u001b[39;00m Y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[39m.\u001b[39mget(\n\u001b[1;32m   1852\u001b[0m     metric, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m ) \u001b[39mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   1854\u001b[0m     \u001b[39m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   1855\u001b[0m     \u001b[39m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   1856\u001b[0m     D_chunk\u001b[39m.\u001b[39mflat[sl\u001b[39m.\u001b[39mstart :: _num_samples(X) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:2022\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m distance\u001b[39m.\u001b[39msquareform(distance\u001b[39m.\u001b[39mpdist(X, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[1;32m   2020\u001b[0m     func \u001b[39m=\u001b[39m partial(distance\u001b[39m.\u001b[39mcdist, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m-> 2022\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1563\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1560\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1562\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1563\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   1565\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         )\n\u001b[0;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    370\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[1;32m    371\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/sklearn/utils/extmath.py:160\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    152\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39m b\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    155\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    156\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    157\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    158\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    159\u001b[0m ):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39;49mtoarray()\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/glg/lib/python3.9/site-packages/scipy/sparse/_compressed.py:1062\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     y \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mT\n\u001b[1;32m   1061\u001b[0m M, N \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39m_swap(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m-> 1062\u001b[0m csr_todense(M, N, x\u001b[39m.\u001b[39;49mindptr, x\u001b[39m.\u001b[39;49mindices, x\u001b[39m.\u001b[39;49mdata, y)\n\u001b[1;32m   1063\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for random_state in [0, 10, 42]: \n",
    "    for n_cluster in range(2,7):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=random_state).fit(shortened_sent_reducded)\n",
    "        labels = kmeans_model.labels_\n",
    "\n",
    "        # print(f'Calinski-Harabasz Index: {calinski_harabasz_score(sample_data, labels)}')\n",
    "        # print(f'Davies-Bouldin Index: {davies_bouldin_score(sample_data, labels)}')\n",
    "        print(f'n_clusters = {n_cluster}, random_state = {random_state}: Silhouette Coefficient: {silhouette_score(shortened_sent_bow, labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('glg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a089b623389f25476dadc88e27c19e01bed25fe38415d8d9feae642f6af10d88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
