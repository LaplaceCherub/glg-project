# üöÄ [GLG](https://glginsights.com/) project

## ü§ù A match made in machine learning heaven: linking every request to the best expert
### üßë‚Äçü§ù‚Äçüßë By [Cris Fortes](https://www.linkedin.com/in/crisfortes/), [Ying Hu](https://www.linkedin.com/in/ying-hu-math/) and [Cody McCormack](https://www.linkedin.com/in/codymccormack/)

<details><summary>Context</summary>
<p>

Cris, Ying and Cody are students of [FourthBrain's](https://fourthbrain.ai/) [Machine Learning Engineer course](https://fourthbrain.ai/courses/machine-learning-engineer/), cohort 9 (August-December 2022). This repository (repo) is part of our capstone project, a required deliverable from our curriculum. For that we've chosen to work on the GLG project.

</p>
</details>
  
<details><summary>The problem</summary>
<p>

GLG's business largely revolves around matching clients, requesting insights on a specific topic, with an expert on that topic from their large database so that they can meet by phone, video or in person. Visually: 

[INSERT BELOW PICTURE OF PROCESS FLOW]

Since GLG receives 100s of these requests per day, how can they leverage machine learning to semi-automate the matching process at scale? 

</p>
</details>
  
<details><summary>The solution</summary>
<p>
  
Natural Language Processing (NLP), consisting of three steps:

- Step 1:  Named-Entity Recognition (NER)
Possible libraries: spaCy, The Natural Language Toolkit (NLTK), TensorFlow, Keras

- Step 2: Hierarchical clustering
Under consideration: decision tree, K-means clustering, Latent Dirichlet allocation (LDA)

- *Step 3: build a recommendation system to suggest the highest matching expert(s) for each request but that is outside the scope of this project

**Illustrative and simplified example**: 

[INSERT BELOW PICTURE OF THREE AFOREMENTIONED STEPS INCLUDING INPUT/MODEL/OUTPUT SEQUENCE]

Acronyms: NLP (Natural Language Processing), NER (Named-Entity Recognition), HC (Hierarchical Clustering), 
DJ (Disc Jockey), GLG (Gerson Lehrman Group). * Step 3 is outside the scope of this project

</p>
</details>
  
<details><summary>Where we've been</summary>
<p>

**Data:**

- Did exploratory data analysis (EDA) on two datasets from Kaggle:

  - Annotated Corpus for Named Entity Recognition | Kaggle 

[INSERT BELOW PICTURE WITH EXAMPLES OF PERFORMED EDA]

</p>
</details>
  
<details><summary>Where we're going</summary>
<p>

**Next step:** train our model using this other 2.7-million news articles dataset:

- [ ] All the News 2.0 - Components

- [ ] [PLACEHOLDER: Establish baseline model through AutoML or a pre-trained model + Document performance report in markdown]

</p>
</details> 

<details><summary>Data and model iteration</summary>
<p>

- [ ] [PLACEHOLDER: Document performance, interpretation, and learnings in markdown]

- [ ] [PLACEHOLDER:Document limitations of your model / data / ML pipeline]

- [ ] [PLACEHOLDER: Restructure GitHub into scripts / modules / submodules]

- [ ] [PLACEHOLDER: Ensure that instructors can easily follow your README.md instructions to deploy your demo locally and in the cloud.]

</p>
</details>
  
<details><summary>MLE Stack</summary>
<p>

- [ ] [Exploratory Data Analysis & Wrangling, Experimentation, Data Engineering Pipeline, Machine Learning Pipeline, Deployment Pipeline]

- [ ] [Maybe consider: Feature Store, Metadata store, Model registry, Model serving, Model Monitoring]

</p>
</details>

<details><summary>Conclusions</summary>
<p>

</p>
</details>

<details><summary>Future work</summary>
<p>

</p>
</details>

<details><summary>Authors and acknowledgment</summary>
<p>

</p>
</details>

<details><summary>License</summary>
<p>

</p>
</details>
