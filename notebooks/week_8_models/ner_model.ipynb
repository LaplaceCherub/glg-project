{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 NER\n",
    "This notebook has the following goals:\n",
    "- To test the accuracy of spaCy's entity predictions\n",
    "- To test the impacts of doing lemmatization before vectorization, as a hyperparameter optimization\n",
    "- To test autoML, and compare it to previous models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports and setup\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tpot import TPOTRegressor\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score as accuracy,\n",
    "    recall_score as recall,\n",
    "    precision_score as precision,\n",
    "    f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataframe built based on previous EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>WordLength</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Non-Punctuation</th>\n",
       "      <th>StopWord</th>\n",
       "      <th>IsNER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag  WordLength  Capital  Non-Punctuation  \\\n",
       "0  Sentence: 1      Thousands  NNS   O           9     True             True   \n",
       "1          NaN             of   IN   O           2    False             True   \n",
       "2          NaN  demonstrators  NNS   O          13    False             True   \n",
       "3          NaN           have  VBP   O           4    False             True   \n",
       "4          NaN        marched  VBN   O           7    False             True   \n",
       "\n",
       "   StopWord  IsNER  \n",
       "0     False      0  \n",
       "1      True      0  \n",
       "2     False      0  \n",
       "3      True      0  \n",
       "4     False      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df = pd.read_csv('../datasets/extended_df.csv')\n",
    "ner_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "ner_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some updates to the \"Sentence #\" column\n",
    "Ensuring every row has a sentence number, and changing that column to an int for use as a numerical feature later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>WordLength</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Non-Punctuation</th>\n",
       "      <th>StopWord</th>\n",
       "      <th>IsNER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #           Word  POS Tag  WordLength  Capital  Non-Punctuation  \\\n",
       "0           1      Thousands  NNS   O           9     True             True   \n",
       "1           1             of   IN   O           2    False             True   \n",
       "2           1  demonstrators  NNS   O          13    False             True   \n",
       "3           1           have  VBP   O           4    False             True   \n",
       "4           1        marched  VBN   O           7    False             True   \n",
       "\n",
       "   StopWord  IsNER  \n",
       "0     False      0  \n",
       "1      True      0  \n",
       "2     False      0  \n",
       "3      True      0  \n",
       "4     False      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df['Sentence #'] = ner_df['Sentence #'].str.replace('Sentence: ','')\n",
    "ner_df['Sentence #'].fillna(method='ffill', inplace=True)\n",
    "ner_df['Sentence #'] = ner_df['Sentence #'].astype('int64')\n",
    "ner_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial splits\n",
    "Establishing X and y DataFrames, and splitting prior to any engineering, so as to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ner_df.drop(columns=['Tag', 'IsNER'])\n",
    "y = ner_df['IsNER']\n",
    "X_train = X[:839270]\n",
    "X_test = X[839270:]\n",
    "y_train = y[:839270]\n",
    "y_test = y[839270:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing metrics \n",
    "Based on Electronics Purchase Prediction notebook from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_metrics(y_true, y_pred):\n",
    "    print(f'Confusion Matrix: \\n{confusion_matrix(y_true, y_pred)}')\n",
    "    print('Accuracy: {:.3f}'.format(accuracy(y_true, y_pred)))\n",
    "    print('Recall: {:.3f}'.format(recall(y_true, y_pred)))\n",
    "    print('Precision: {:.3f}'.format(precision(y_true, y_pred)))\n",
    "    print('F1 Score: {:.3f}'.format(f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test One\n",
    "Testing the accuracy of spaCy predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_model(df):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    return_list = []\n",
    "    for sentence in range(df['Sentence #'].max()):\n",
    "        words = nlp(Doc(nlp.vocab, df[df['Sentence #'] == sentence + 1].Word.values))\n",
    "        for word in words:\n",
    "            is_ner = str(word) in set(ent.text for ent in words.ents)\n",
    "            return_list.append((is_ner))\n",
    "    return pd.Series(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[182272   4559]\n",
      " [  8553  13921]]\n",
      "Accuracy: 0.937\n",
      "Recall: 0.619\n",
      "Precision: 0.753\n",
      "F1 Score: 0.680\n"
     ]
    }
   ],
   "source": [
    "preds = spacy_model(X_test)\n",
    "display_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2\n",
    "Testing impacts of lemmatization before vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model from baseline, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cody/anaconda3/envs/glg-project/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:09] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Display metrics for XGB with one-hot encoding:\n",
      "Confusion Matrix: \n",
      "[[180446   6385]\n",
      " [  2111  20363]]\n",
      "Accuracy: 0.959\n",
      "Recall: 0.906\n",
      "Precision: 0.761\n",
      "F1 Score: 0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cody/anaconda3/envs/glg-project/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display metrics for Logistic Regression with one-hot encoding:\n",
      "Confusion Matrix: \n",
      "[[178538   8293]\n",
      " [  3668  18806]]\n",
      "Accuracy: 0.943\n",
      "Recall: 0.837\n",
      "Precision: 0.694\n",
      "F1 Score: 0.759\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(random_state=42)\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "models = [xgb_model, lr_model]\n",
    "model_names = ['XGB', 'Logistic Regression']\n",
    "\n",
    "categorical_cols = ['Word', 'POS']\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64', 'bool']]\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)\n",
    "                                ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)  \n",
    "\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    print('Display metrics for {} with one-hot encoding:'.format(model_name))\n",
    "    display_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retesting with balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display metrics for Logistic Regression with one-hot encoding:\n",
      "Confusion Matrix: \n",
      "[[175263  11568]\n",
      " [  2001  20473]]\n",
      "Accuracy: 0.935\n",
      "Recall: 0.911\n",
      "Precision: 0.639\n",
      "F1 Score: 0.751\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "models = [lr_model]\n",
    "model_names = ['Logistic Regression']\n",
    "\n",
    "categorical_cols = ['Word', 'POS']\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64', 'bool']]\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)\n",
    "                                ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)  \n",
    "\n",
    "    preds = pipeline.predict(X_test)\n",
    "\n",
    "    print('Display metrics for {} with one-hot encoding:'.format(model_name))\n",
    "    display_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(df):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    return_list = []\n",
    "    for sentence in range(df['Sentence #'].max()):\n",
    "        words = nlp(Doc(nlp.vocab, df[df['Sentence #'] == sentence + 1].Word.values))\n",
    "        for word in words:\n",
    "            lemma = word.lemma_\n",
    "            return_list.append((lemma))\n",
    "    return pd.Series(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model from baseling with lemmatization before vectorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cody/anaconda3/envs/glg-project/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:25:57] WARNING: /var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_44tbtwf8c1/croots/recipe/xgboost-split_1659548960882/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Display metrics for XGB with one-hot encoding:\n",
      "Confusion Matrix: \n",
      "[[177469   9362]\n",
      " [  2112  20362]]\n",
      "Accuracy: 0.945\n",
      "Recall: 0.906\n",
      "Precision: 0.685\n",
      "F1 Score: 0.780\n",
      "Display metrics for Logistic Regression with one-hot encoding:\n",
      "Confusion Matrix: \n",
      "[[175626  11205]\n",
      " [  2232  20242]]\n",
      "Accuracy: 0.936\n",
      "Recall: 0.901\n",
      "Precision: 0.644\n",
      "F1 Score: 0.751\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(random_state=42)\n",
    "lr_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "models = [xgb_model, lr_model]\n",
    "model_names = ['XGB', 'Logistic Regression']\n",
    "X_train_lemma = X_train.copy()\n",
    "X_test_lemma = X_test.copy()\n",
    "\n",
    "X_train_lemma['Word'] = lemmatizer(X_train_lemma)\n",
    "X_test_lemma['Word'] = lemmatizer(X_test_lemma)\n",
    "categorical_cols = ['Word', 'POS']\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64', 'bool']]\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('model', model)\n",
    "                                ])\n",
    "\n",
    "    pipeline.fit(X_train_lemma, y_train)  \n",
    "\n",
    "    preds = pipeline.predict(X_test_lemma)\n",
    "\n",
    "    print('Display metrics for {} with one-hot encoding:'.format(model_name))\n",
    "    display_metrics(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3\n",
    "Testing TPOT for autoML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# X_train_auto = X_train.copy()\n",
    "# ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "# X_train_auto = ohe.fit_transform(X_train_auto)\n",
    "# tpot = TPOTRegressor(generations=10, \n",
    "#                      population_size=40,\n",
    "#                      scoring='accuracy', \n",
    "#                      verbosity=2,\n",
    "#                      random_state=42,\n",
    "#                      config_dict='TPOT sparse')\n",
    "# tpot.fit(X_train_auto, y_train)\n",
    "# print(f\"Tpop score on test data: {tpot.score(test_features, test_labels):.2f}\")\n",
    "# tpot.export('tpot_mpg_pipeline.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('glg-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f13607be891010820e0da429067f1a535e7a2044f342673b34a4d987fe81d266"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
