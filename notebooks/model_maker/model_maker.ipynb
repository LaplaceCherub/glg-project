{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from nltk import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df = pd.read_csv('../../datasets/extended_df.csv')\n",
    "ner_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "ner_df['Sentence #'] = ner_df['Sentence #'].str.replace('Sentence: ','')\n",
    "ner_df['Sentence #'].fillna(method='ffill', inplace=True)\n",
    "ner_df['Sentence #'] = ner_df['Sentence #'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = ner_df.groupby('Sentence #', as_index=False)['Word'].apply(lambda x:x.str.cat(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def sentence_preprocessor(df):\n",
    "    return_list = []\n",
    "    for sentence in range(df['Sentence #'].max()):\n",
    "        words = nlp(Doc(nlp.vocab, df[df['Sentence #'] == sentence + 1].Word.values))\n",
    "        for word in words:\n",
    "            # print(word)\n",
    "            word_base = word\n",
    "            word_lemma = word.lemma_\n",
    "            word_pos = word.pos_\n",
    "            word_tag = word.tag_\n",
    "            word_dep = word.dep_\n",
    "            word_length = len(word)\n",
    "            word_capitalization = str(word)[0].isupper()\n",
    "            word_punctiation = str(word).isalnum()\n",
    "            word_stop = word.is_stop\n",
    "            is_ner = str(word) in set(ent.text for ent in words.ents)\n",
    "            return_list.append((word_base, word_lemma, word_pos, word_tag, word_dep, word_length, word_capitalization, word_punctiation, word_stop, is_ner))\n",
    "    return return_list\n",
    "\n",
    "df_list = sentence_preprocessor(ner_df)\n",
    "intermediate_df = pd.DataFrame(df_list, columns=['WordBase', 'WordLemma', 'WordPOS', 'WordTag', \n",
    "    'WordDep', 'WordLength', 'IsCapitalized', 'NonPunctuation', 'IsStop', 'PossibleNER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = intermediate_df[:835700]\n",
    "X_test = intermediate_df[835700:]\n",
    "y_train = ner_df.IsNER[:835700]\n",
    "y_test = ner_df.IsNER[835700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;constant&#x27;),\n",
       "                                                  [&#x27;IsCapitalized&#x27;,\n",
       "                                                   &#x27;NonPunctuation&#x27;, &#x27;IsStop&#x27;,\n",
       "                                                   &#x27;PossibleNER&#x27;,\n",
       "                                                   &#x27;WordLength&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;WordLemma&#x27;, &#x27;WordPOS&#x27;,\n",
       "                                                   &#x27;WordTag&#x27;, &#x27;WordDep&#x27;])]...\n",
       "                               feature_types=None, gamma=0, gpu_id=-1,\n",
       "                               grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "                               interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "                               max_delta_step=0, max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=42, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;constant&#x27;),\n",
       "                                                  [&#x27;IsCapitalized&#x27;,\n",
       "                                                   &#x27;NonPunctuation&#x27;, &#x27;IsStop&#x27;,\n",
       "                                                   &#x27;PossibleNER&#x27;,\n",
       "                                                   &#x27;WordLength&#x27;]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                                  (&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;WordLemma&#x27;, &#x27;WordPOS&#x27;,\n",
       "                                                   &#x27;WordTag&#x27;, &#x27;WordDep&#x27;])]...\n",
       "                               feature_types=None, gamma=0, gpu_id=-1,\n",
       "                               grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "                               interaction_constraints=&#x27;&#x27;,\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "                               max_delta_step=0, max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;,\n",
       "                               random_state=42, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, SimpleImputer(strategy=&#x27;constant&#x27;),\n",
       "                                 [&#x27;IsCapitalized&#x27;, &#x27;NonPunctuation&#x27;, &#x27;IsStop&#x27;,\n",
       "                                  &#x27;PossibleNER&#x27;, &#x27;WordLength&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;WordLemma&#x27;, &#x27;WordPOS&#x27;, &#x27;WordTag&#x27;,\n",
       "                                  &#x27;WordDep&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;IsCapitalized&#x27;, &#x27;NonPunctuation&#x27;, &#x27;IsStop&#x27;, &#x27;PossibleNER&#x27;, &#x27;WordLength&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;WordLemma&#x27;, &#x27;WordPOS&#x27;, &#x27;WordTag&#x27;, &#x27;WordDep&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, feature_types=None, gamma=0, gpu_id=-1,\n",
       "              grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_bin=256, max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=42, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  SimpleImputer(strategy='constant'),\n",
       "                                                  ['IsCapitalized',\n",
       "                                                   'NonPunctuation', 'IsStop',\n",
       "                                                   'PossibleNER',\n",
       "                                                   'WordLength']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['WordLemma', 'WordPOS',\n",
       "                                                   'WordTag', 'WordDep'])]...\n",
       "                               feature_types=None, gamma=0, gpu_id=-1,\n",
       "                               grow_policy='depthwise', importance_type=None,\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "                               max_delta_step=0, max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=42, ...))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "categorical_cols = ['WordLemma', 'WordPOS', 'WordTag', 'WordDep']\n",
    "numerical_cols = ['IsCapitalized', 'NonPunctuation', 'IsStop', 'PossibleNER', 'WordLength']\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('xgb_model', xgb_model)\n",
    "                            ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pipeline, open('ner_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = pd.read_csv('../../datasets/ner_dataset.csv', \n",
    "    encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['Sentence #'] = ner_dataset['Sentence #'].str.replace('Sentence:', '')\n",
    "ner_dataset = ner_dataset.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['Sentence #'] = ner_dataset['Sentence #'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = ner_dataset.groupby('Sentence #', as_index=False)['Word'].apply(lambda x: x.str.cat(sep=' '))\n",
    "sentences_df = sentences_df.rename(columns={'Word': 'Sentences'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    8412\n",
       "Sentences      The\n",
       "Name: 8411, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.iloc[8411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df = sentences_df.drop(labels=[8411], axis=0)\n",
    "sentences_df = sentences_df.reset_index()\n",
    "sentences_df = sentences_df.drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/cody/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/cody/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/cody/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cody/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/cody/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lemmatize_words(sentence):\n",
    "    wordnet_map = {'N':wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(sentence.split())\n",
    "    return ' '.join([WordNetLemmatizer().lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))\n",
    "                    for word, pos in pos_tagged_text])\n",
    "    \n",
    "def lda_sent_process(text):\n",
    "    text = text.lower()  \n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in STOPWORDS])\n",
    "    text = _lemmatize_words(text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df['lda_sents'] = sentences_df['Sentences'].apply(lambda x: lda_sent_process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary \n",
    "from gensim import models \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(sentences_df['lda_sents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dct.doc2bow(sentence) for sentence in sentences_df['lda_sents']]\n",
    "lda = models.LdaModel(corpus, num_topics=15, random_state=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['vote', 'bird', 'flu', 'last', 'say', 'month', 'election', 'since', 'confirm', 'case']\n",
      "Topic 1:  ['minister', 'say', 'prime', 'mr', 'president', 'official', 'government', 'foreign', 'chavez', 'tuesday']\n",
      "Topic 2:  ['foreign', 'east', 'beijing', 'france', 'britain', 'russia', 'gas', 'russian', 'french', 'august']\n",
      "Topic 3:  ['say', 'kill', 'attack', 'u', 'force', 'military', 'official', 'bomb', 'afghanistan', 'iraqi']\n",
      "Topic 4:  ['say', 'iran', 'united', 'state', 'nuclear', 'program', 'country', 'nation', 'international', 'un']\n",
      "Topic 5:  ['say', 'official', 'police', 'news', 'arrest', 'city', 'report', 'force', 'spokesman', 'muslim']\n",
      "Topic 6:  ['party', 'election', 'new', 'opposition', 'president', 'rule', 'mr', 'political', 'leader', 'call']\n",
      "Topic 7:  ['say', 'woman', 'result', 'kilometer', 'saudi', 'see', 'make', 'get', 'show', 'heavy']\n",
      "Topic 8:  ['people', 'say', 'kill', 'least', 'israeli', 'two', 'militant', 'attack', 'death', 'official']\n",
      "Topic 9:  ['world', 'year', 'high', 'economic', 'economy', 'price', 'million', 'percent', 'country', 'say']\n",
      "Topic 10:  ['thousand', 'claim', 'year', 'lebanon', 'un', 'one', 'responsibility', 'group', 'people', 'say']\n",
      "Topic 11:  ['president', 'government', 'israel', 'palestinian', 'say', 'mr', 'peace', 'leader', 'end', 'talk']\n",
      "Topic 12:  ['state', 'u', 'united', 'secretary', '11', 'hurricane', 'storm', '2001', 'island', 'emergency']\n",
      "Topic 13:  ['say', 'u', 'president', 'bush', 'charge', 'mr', 'right', 'court', 'former', 'house']\n",
      "Topic 14:  ['oil', 'series', 'market', 'power', 'company', 'production', 'decline', 'voa', 'government', 'long']\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "  key_indices = re.findall(r'\"(.*?)\"', topic[1])\n",
    "  key_words = [dct[int(idx)] for idx in key_indices]\n",
    "  print(f'Topic {topic[0]}: ', key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = {}\n",
    "for num, topics in enumerate(topics):\n",
    "  key_indices = re.findall(r'\"(.*?)\"', topic[1])\n",
    "  key_words = [dct[int(idx)] for idx in key_indices]\n",
    "  topics_dict[num] = key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = {\n",
    "  0: ['vote', 'bird', 'election', 'flu'], \n",
    "  1: ['minister', 'prime', 'north', 'south', 'president', 'korea'],\n",
    "  2: ['foreign', 'Beijing', 'Britain', 'France', 'gas', 'German', 'Middle', 'East', 'Russian'], \n",
    "  3: ['kill', 'attack', 'military', 'bomb', 'force'],\n",
    "  4: ['Iran', 'United', 'State', 'nuclear', 'European', 'international'],\n",
    "  5: [ 'police', 'force', 'city', 'Muslim', 'spokesman', 'security' ], \n",
    "  6: ['party', 'president', 'election', 'leader'], \n",
    "  7: ['woman', 'citizen', 'explosive'],\n",
    "  8: ['Israeli', 'death', 'kill', 'Islamic', 'militant'],\n",
    "  9: ['world', 'economic', 'economy', 'price', 'country'],\n",
    "  10: ['Lebanon', 'blast', 'responsibility', 'explosion', 'group'],\n",
    "  11: ['government', 'president', 'Israel', 'Palestinian','peace', 'leader'], \n",
    "  12: ['United', 'State', 'secretary', 'storm', 'hurricane', 'emergency'],\n",
    "  13: ['president', 'charge', 'right', 'court', 'Iraq', 'house'],\n",
    "  14: ['oil', 'company',  'market', 'demand', 'production', 'decline', 'power', 'government'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(new_text, lda_model, dct): \n",
    "  '''\n",
    "  new_text: str\n",
    "  lda_model: load from lda.pkl\n",
    "  dct: load from dct.pkl\n",
    "  '''\n",
    "  new_text_doc = lda_sent_process(new_text)\n",
    "  topics = lda_model[dct.doc2bow(new_text_doc)]\n",
    "  for num, topic in enumerate(topics): \n",
    "    print(f'Topic {topic[0]}:  with probability {topic[1]}')\n",
    "    print(topics_dict[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dct.pkl', 'wb') as pickle_dict: \n",
    "  pickle.dump(dct, pickle_dict)\n",
    "with open('lda.pkl', 'wb') as pickle_lda:\n",
    "  pickle.dump(lda, pickle_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (0.10.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (4.24.0)\n",
      "Requirement already satisfied: nltk in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: scikit-learn in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (1.1.3)\n",
      "Requirement already satisfied: scipy in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (1.9.3)\n",
      "Requirement already satisfied: sentencepiece in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: torchvision in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: tqdm in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (1.23.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from sentence-transformers) (1.13.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: click in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cody/anaconda3/envs/test_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences_df['Sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors \n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_near_sent(text, emb_model, knn_model):\n",
    "  embedding = emb_model.encode([text])\n",
    "  _, index = knn_model.kneighbors(embedding)\n",
    "  for idx in range(index.shape[1]):\n",
    "    print(f'{idx+1}.',  sentences_df['Sentences'][index[0,idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "2. Thousands of anti-war protesters have marched in London joining protests in Japan , Australia and elsewhere in the world ahead of the third anniversary of the U.S.-led invasion of Iraq .\n",
      "3. Thousands of people in cities across Britain have rallied to protest Israeli military action in Lebanon .\n"
     ]
    }
   ],
   "source": [
    "get_near_sent(sentences_df['Sentences'][0], emb_model=model, knn_model=nbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_model.pkl', 'wb') as pickle_emb:\n",
    "  pickle.dump(model, pickle_emb)\n",
    "with open('knn_modle.pkl', 'wb') as pickle_knn:\n",
    "  pickle.dump(nbrs, pickle_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75668520ec9d93b2e72f3e56da66b5d79609c343e9e9b14938cb1a6846b858c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
